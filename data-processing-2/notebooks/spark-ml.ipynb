{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING WITH SPARK\n",
    "\n",
    "### Is it a reddit or twitter post?\n",
    "\n",
    "We decided to do classification with spark. Since we couldn't label if tweet is pro-trump or pro-biden, which was our main idea, we decided to classify texts in reddit posts and tweets. <br>\n",
    "and try to predict if some given text is whether it came from reddit or twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependecies for whole notebook\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id, desc, udf, lit, rand, when, col\n",
    "import re\n",
    "\n",
    "\n",
    "# ML\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext and SqlContext\n",
    "sc = SparkContext().getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading our reddit data\n",
    "data_r = sqlContext.read.csv(\n",
    "    \"./data/reddit.csv\",\n",
    "    header=True,\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    multiLine=True,\n",
    "    inferSchema=True,\n",
    "    sep=',',\n",
    "    escape='\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Is there a bettin...|  0.0|\n",
      "|DC and PR should ...|  0.0|\n",
      "|Good statement Mi...|  0.0|\n",
      "|I was wondering i...|  0.0|\n",
      "|Usually that's th...|  0.0|\n",
      "|Yeah, I honestly ...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80621"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_r = data_r.select('comment')\n",
    "data_r = data_r.filter(\"comment != '[removed]'\") # for some reason sql \"or\" doesnt work so we splited this into two steps -> some redit posts were deleted or removed\n",
    "data_r = data_r.filter(\"comment != '[deleted]'\")\n",
    "data_r = data_r.selectExpr(\"comment as text\")\n",
    "data_r = data_r.withColumn('label', lit(0.0))\n",
    "data_r.show(6)\n",
    "data_r.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|@TscookT your Twe...|  1.0|\n",
      "|@HollejHolle @Mat...|  1.0|\n",
      "|@chen88888899 Tru...|  1.0|\n",
      "|@thehorrorchick S...|  1.0|\n",
      "|@jamesechoeson @e...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80621"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading our twitter data\n",
    "data_tw = sqlContext.read.csv(\n",
    "    \"./data/tweets.csv\",\n",
    "    header=True,\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    multiLine=True,\n",
    "    inferSchema=True,\n",
    "    sep=',',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "data_tw = data_tw.select('tweet_text').limit(data_r.count()) # we want the same amout of rows as in reddit dataframe\n",
    "data_tw = data_tw.selectExpr('tweet_text as text')\n",
    "data_tw = data_tw.withColumn('label', lit(1.0))\n",
    "data_tw.show(5)\n",
    "data_tw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161242"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_tw.union(data_r) # appending dataframes\n",
    "# head\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets clean our text column\n",
    "def cleaner(text):\n",
    "    \"\"\"Removes mentiones, links and hashtags from text\"\"\"\n",
    "    \n",
    "    if text is None: # in order for select method to work we must escape NoneType case\n",
    "        return 0\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    cleaned = []\n",
    "    for word in words:\n",
    "        if '@' in word or word.startswith('https') or word.startswith('#') or word == 'RT':\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            cleaned.append(word)\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_udf = udf(cleaner)\n",
    "cleaned = df.withColumn('text', remove_udf(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|your Tweets are g...|  1.0|\n",
      "|Their no match fo...|  1.0|\n",
      "|     Trump will win.|  1.0|\n",
      "|SON OF A BITCH. B...|  1.0|\n",
      "|My Grandmother su...|  1.0|\n",
      "|After Failed Atte...|  1.0|\n",
      "|This is excellent...|  1.0|\n",
      "|ðŸš¨ðŸš¨ðŸš¨\n",
      "Homeland S...|  1.0|\n",
      "|This brought me t...|  1.0|\n",
      "|With that said Pr...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|That was a great ...|  0.0|\n",
      "|      Had--&gt; lost|  0.0|\n",
      "|Trump ran out of ...|  1.0|\n",
      "|Mini tree is for ...|  1.0|\n",
      "|Meltzer being ant...|  1.0|\n",
      "|Canâ€™t wait to see...|  1.0|\n",
      "|CNN has had it wi...|  0.0|\n",
      "|Am I the only one...|  1.0|\n",
      "|Ah yes Lady Gaga!...|  0.0|\n",
      "|If you canâ€™t beat...|  0.0|\n",
      "|Fauci and the CDC...|  0.0|\n",
      "|Following Trump's...|  1.0|\n",
      "|. Should have bou...|  1.0|\n",
      "|Joe Biden es un c...|  1.0|\n",
      "|TRUMP IS YOUR PRE...|  1.0|\n",
      "|I like the way Bi...|  1.0|\n",
      "|It's called diplo...|  0.0|\n",
      "|The impeachment t...|  0.0|\n",
      "|Shouldn't it be t...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 19 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets randomize order of texts, so we have shuffeled dataframe for later spliting of training and testing datasets, since randomSplit() doesnt work sometimes\n",
    "shuffle = cleaned.orderBy(rand())\n",
    "shuffle.show(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|That was a great ...|  0.0|\n",
      "|      Had--&gt; lost|  0.0|\n",
      "|Trump ran out of ...|  1.0|\n",
      "|Mini tree is for ...|  1.0|\n",
      "|Meltzer being ant...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Trump WON 79Milli...|  1.0|\n",
      "|It is despicable ...|  1.0|\n",
      "|Georgia prosecuto...|  1.0|\n",
      "|Basically the lef...|  1.0|\n",
      "|No, Trump is NOT ...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spliting without randomSplit() -> since randomSplit() sometimes works and sometimes it returns blank columns\n",
    "training_df = shuffle.limit(110000)\n",
    "test_df = shuffle.subtract(training_df)\n",
    "\n",
    "training_df.show(5)\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Donald Trump itâ€™s...|  1.0|\n",
      "|He didnt help Bid...|  1.0|\n",
      "|No but there has ...|  0.0|\n",
      "|Hi `Mamacrass`. T...|  0.0|\n",
      "|Wow Mitch Mcconne...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.orderBy(rand()) # we shuffled it because for some reason after subtract method we get sorted df, so we just wanted to double check\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "First we will apply naive bayes algorithm, this code is mostly taken from lecture slides since this code was our starting point into ML with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building our pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf\")\n",
    "nb = NaiveBayes()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            nb])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 1.0]).build()\n",
    "\n",
    "# Crossvalidation -> if we understood it right, with this approach we will train our model through crossvalidation so it kinda gets \"double checked\" and can perform better than without cross validation\n",
    "cv = CrossValidator(estimator = pipeline, \n",
    "                    estimatorParamMaps = paramGrid,\n",
    "                    evaluator = MulticlassClassificationEvaluator(),\n",
    "                    numFolds = 2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model started\n",
      "Model has finished\n"
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "print('Model started')\n",
    "cvModel = cv.fit(training_df)\n",
    "print('Model has finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|Donald Trump itâ€™s...|  1.0|       1.0|\n",
      "|He didnt help Bid...|  1.0|       1.0|\n",
      "|No but there has ...|  0.0|       0.0|\n",
      "|Hi `Mamacrass`. T...|  0.0|       0.0|\n",
      "|Wow Mitch Mcconne...|  0.0|       0.0|\n",
      "|Ted Cruz For his ...|  1.0|       1.0|\n",
      "|Probably need to ...|  1.0|       1.0|\n",
      "|Ignore that apolo...|  0.0|       0.0|\n",
      "|Which is why ther...|  0.0|       0.0|\n",
      "|New Capitol Video...|  1.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running model on test data\n",
    "result = cvModel.transform(test_df)\n",
    "result.select(\"text\", \"label\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8255926361802286\n"
     ]
    }
   ],
   "source": [
    "# Evaluating our results\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy:', evaluator.evaluate(result, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.8256098029596373\n"
     ]
    }
   ],
   "source": [
    "print('f1: ', evaluator.evaluate(result, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In the code below we will apply logistic regression model to help us distinguish between reddit and twitter posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset len: 130000\n",
      "Test Dataset len: 28975\n"
     ]
    }
   ],
   "source": [
    "# Building our pipeline\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\", \"s\", '&gt']\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer,\n",
    "                            stopwordsRemover,\n",
    "                            countVectors])\n",
    "\n",
    "\n",
    "# pushing dataframe through pipeline\n",
    "pipelineFit = pipeline.fit(shuffle)\n",
    "dataset = pipelineFit.transform(shuffle)\n",
    "\n",
    "training_df = dataset.limit(130000)\n",
    "test_df = dataset.subtract(training_df)\n",
    "\n",
    "\n",
    "print('Training Dataset len:', training_df.count()) # we will use same data we already splitted\n",
    "print('Test Dataset len:', test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and training our model without cross validation\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(training_df)\n",
    "\n",
    "# Running model on test df\n",
    "predictions_df = lrModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|Bidenâ€™s propagand...|  1.0|       1.0|\n",
      "|I hope to god tha...|  0.0|       0.0|\n",
      "|What a coincidenc...|  1.0|       1.0|\n",
      "|It should be done...|  1.0|       1.0|\n",
      "|Tomorrow is the *...|  1.0|       1.0|\n",
      "|He went to the WH...|  1.0|       1.0|\n",
      "|What a waste of s...|  1.0|       1.0|\n",
      "|Listen if Bidenâ€™s...|  1.0|       0.0|\n",
      "|I can give you de...|  1.0|       1.0|\n",
      "|It's almost as if...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df.select('text', 'label','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8951164797239\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy:', evaluator.evaluate(predictions_df, {evaluator.metricName: \"accuracy\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
