{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-80ebb99b-d6f8-4061-b0d0-8d7f67b8ee3f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# DATA EXTRACTION NOTEBOOK\n",
    "\n",
    "This notebook consists of 4 parts we used for extraction of data that we were interested in.\n",
    "\n",
    "   * Twitter Stream/Present Data\n",
    "   * Twitter Historic Data\n",
    "   * Reddit Stream/Present Data\n",
    "   * Reddit Historic Data\n",
    "  \n",
    "We will explain each part separately under its title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDECIES\n",
    "import tweepy\n",
    "import praw\n",
    "import sqlite3\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from IPython import display\n",
    "import getpass\n",
    "# import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWITTER API AUTHENTICATION VARIABLES\n",
    "from config import twitter_config\n",
    "\n",
    "tw_client_key = twitter_config['CLIENT_KEY']\n",
    "tw_client_secret = twitter_config['CLIENT_SECRET']\n",
    "tw_access_key = twitter_config['RESOURCE_OWNER_KEY']\n",
    "tw_access_secret = twitter_config['RESOURCE_OWNER_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDDIT API AUTHENTICATION VARIABLES\n",
    "# W\n",
    "from config import reddit_config\n",
    "\n",
    "rd_client_key = reddit_config['CLIENT_KEY']\n",
    "rd_client_secret = reddit_config['CLIENT_SECRET']\n",
    "rd_user_agent = reddit_config['AGENT']\n",
    "rd_user_name = reddit_config['USER_NAME']\n",
    "rd_pw = getpass.getpass('Enter reddit password:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Stream Data\n",
    "We extracted only 'pure' tweets, not retweets or quoted tweets, since we think that these are real opinions of people. We saved every tweet and it's properties into a database. We also included only tweets that are written in english and tweets that are written by users that have 'USA' in their profile location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-20a4cb07-71e8-4a55-8bd8-c26382e5cd66",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16365195,
    "execution_start": 1612282309271,
    "output_cleared": false,
    "source_hash": "3b7ab8cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell won't stop running unless interrupted\n",
    "\n",
    "# Database connection\n",
    "conn = sqlite3.connect('data/db.sqlite3')\n",
    "cur = conn.cursor()\n",
    "\n",
    "class OurStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        api_response = status._json\n",
    "        # we take only tweets that are not retweets or quote tweets and come from USA -> ideally we remove tweets that are replies so we get 'pure opinion' as first tweet\n",
    "        if status.user.location:\n",
    "            if 'USA' in status.user.location and not hasattr(status, 'retweeted_status') and status.is_quote_status == False:\n",
    "            \n",
    "                # We analysed what attributes we got from couple status objects so we can filter out tweets even more\n",
    "                # attrs = inspect.getmembers(status, lambda a:not(inspect.isroutine(a)))\n",
    "                # for at in attrs:\n",
    "                #     print(at)\n",
    "                #     print('-' * 50)\n",
    "                \n",
    "                # tweet related\n",
    "                id_status = status.id\n",
    "                tw_created_at = status.created_at\n",
    "                if not status.truncated: # if tweet is not wrapped take text attribute as text, else take extended_tweet['full_text'] as text\n",
    "                    text = status.text\n",
    "                else:\n",
    "                    text = status.extended_tweet['full_text']\n",
    "                \n",
    "                # user related\n",
    "                user_name = status.user.screen_name\n",
    "                user_created_at = status.user.created_at\n",
    "                profile_bio = status.user.description\n",
    "                followers = status.user.followers_count\n",
    "                user_location = status.user.location\n",
    "\n",
    "\n",
    "                cur.execute(\"\"\"INSERT INTO tweets(\n",
    "                    tweet_id,\n",
    "                    tweet_created_at,\n",
    "                    tweet_text,\n",
    "                    user_name,\n",
    "                    user_created_at,\n",
    "                    profile_bio,\n",
    "                    followers,\n",
    "                    user_location)\n",
    "                    values(?,?,?,?,?,?,?,?)\"\"\",(\n",
    "                    id_status,\n",
    "                    tw_created_at,\n",
    "                    text, user_name,\n",
    "                    user_created_at,\n",
    "                    profile_bio,\n",
    "                    followers,\n",
    "                    user_location)\n",
    "                )\n",
    "                \n",
    "                conn.commit()\n",
    "                print(id_status)\n",
    "                display.clear_output(wait=True)\n",
    "                \n",
    "                \n",
    "#                 print('commited') # uncomment to see if it's working\n",
    "\n",
    "def main():\n",
    "    auth = tweepy.OAuthHandler(tw_client_key, tw_client_secret)\n",
    "    auth.set_access_token(tw_access_key, tw_access_secret)\n",
    "\n",
    "    stream_listener = OurStreamListener()\n",
    "    OurStream = tweepy.Stream(auth=auth, listener=stream_listener, tweet_mode='extended')\n",
    "    OurStream.filter(track=['trump', 'biden'])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        conn.close() # when interrupted it will close db connection to prevent locking of db and exit the program\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Historic Data\n",
    "Since our twitter developer accounts are limited to only 5000 historic tweets we had to re-run this piece of code 4 times with different credentials in order to get as much data as possible. This didn't go as planned because accounts provided by you couldn't set up custom application envrionments and that was needed for retreiving historic data from twitter. <br> We only mnaged to get around 3k tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Database connection\n",
    "conn = sqlite3.connect('data/db.sqlite3')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Twitter Authentication with tweepy\n",
    "auth = tweepy.OAuthHandler(tw_client_key, tw_client_secret)\n",
    "auth.set_access_token(tw_access_key, tw_access_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler=auth)\n",
    "\n",
    "# querying history tweets - Capitol Storming Day - 06.01.2021\n",
    "history = tweepy.Cursor(api.search_full_archive,\n",
    "                        environment_name='dev',\n",
    "                        query='trump OR biden lang:en', # -is:retweet cant exclude retweets, premium feature\n",
    "                        fromDate='202101062200',\n",
    "                        toDate='202101062300').items(100) # 5000 max out\n",
    "\n",
    "# Iterating over tweepy cursor and saving each status into db -> tweepy cursor makes pagination easy\n",
    "counter = 1\n",
    "\n",
    "for status in history:\n",
    "    \n",
    "    # tweet related\n",
    "    id_status = status.id\n",
    "    tw_created_at = status.created_at\n",
    "    if not status.truncated: # if tweet is not wrapped take text attribute as text, else take extended_tweet['full_text'] as text\n",
    "        text = status.text\n",
    "    else:\n",
    "        text = status.extended_tweet['full_text']\n",
    "\n",
    "    # user related\n",
    "    user_name = status.user.screen_name\n",
    "    user_created_at = status.user.created_at\n",
    "    profile_bio = status.user.description\n",
    "    followers = status.user.followers_count\n",
    "    user_location = status.user.location\n",
    "\n",
    "    cur.execute(\"\"\"INSERT INTO tweets(\n",
    "        tweet_id,\n",
    "        tweet_created_at,\n",
    "        tweet_text,\n",
    "        user_name,\n",
    "        user_created_at,\n",
    "        profile_bio,\n",
    "        followers,\n",
    "        user_location)\n",
    "        values(?,?,?,?,?,?,?,?)\"\"\",\n",
    "        (\n",
    "        id_status,\n",
    "        tw_created_at,\n",
    "        text, user_name,\n",
    "        user_created_at,\n",
    "        profile_bio,\n",
    "        followers,\n",
    "        user_location)\n",
    "    )\n",
    "    \n",
    "    print(counter)\n",
    "    display.clear_output(wait=True)\n",
    "    counter += 1\n",
    "    \n",
    "# commit and close\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Stream Data\n",
    "\n",
    "For streaming reddit data we used PRAW library, it's pretty straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-f80e76d3-7c73-4340-8efc-95f140e4f138",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell won't stop running unless interrupted\n",
    "\n",
    "# PRAW API object\n",
    "reddit = praw.Reddit(client_id=rd_client_key,\n",
    "                     client_secret=rd_client_secret,\n",
    "                     password=rd_pw,\n",
    "                     user_agent=rd_user_agent,\n",
    "                     username=rd_user_name,\n",
    "                     check_for_async=False)\n",
    "\n",
    "reddit.read_only = True # We dont want to publish anything so we will use read only mode\n",
    "\n",
    "# Database connection\n",
    "conn = sqlite3.connect('data/db.sqlite3')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# STREAM\n",
    "def main():\n",
    "    counter = 1\n",
    "    for comment in reddit.subreddit('politics').stream.comments():\n",
    "        id = comment.id\n",
    "        author = comment.author.name\n",
    "        comment_txt = comment.body\n",
    "        score = comment.score\n",
    "        pinned = comment.stickied\n",
    "        created_at = datetime.datetime.fromtimestamp(comment.created_utc) # returns UNIX epoch time so we need to convert it\n",
    "        \n",
    "        try:\n",
    "            cur.execute(\"\"\"INSERT INTO reddit(\n",
    "                id,\n",
    "                author,\n",
    "                comment,\n",
    "                score,\n",
    "                pinned,\n",
    "                created_at) values(?,?,?,?,?, ?)\"\"\",\n",
    "                (id,\n",
    "                author,\n",
    "                comment_txt,\n",
    "                score,\n",
    "                pinned,\n",
    "                created_at)\n",
    "            )\n",
    "            \n",
    "            conn.commit()\n",
    "            print(counter)\n",
    "            display.clear_output(wait=True) # it will show us how many comments we scrape\n",
    "            counter += 1\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Error:', e.text)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        conn.close() # when interrupted it will close db connection to prevent locking of db and exit the program\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Historic Data\n",
    "For historic reddit data we had to use pushshift.io API endpoints. <br>\n",
    "We defined a function that can get maximum 100 entries per request, then we will make a loop that will call that function for each minute in a given timeframe we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(lower_timestamp, upper_timestamp, subreddit, size):\n",
    "    batch = []\n",
    "    api_url = f\"https://api.pushshift.io/reddit/search/comment/?after={lower_timestamp}&before={upper_timestamp}&sort_type=score&sort=desc&subreddit={subreddit}&size={size}\"\n",
    "\n",
    "    try:\n",
    "        with requests.get(api_url) as req:\n",
    "            data = req.json()['data']\n",
    "#             print(len(data), datetime.datetime.fromtimestamp(data[0]['created_utc'])) # prints out len of batch extracted (max is 100) and date of first comment -> if it increases by minute then it means it works properly\n",
    "        \n",
    "        with sqlite3.connect('data/db.sqlite3') as conn:\n",
    "            cur = conn.cursor()\n",
    "            for entry in data:\n",
    "                id = entry['id']\n",
    "                author = entry['author']\n",
    "                comment_txt = entry['body']\n",
    "                created_at = datetime.datetime.fromtimestamp(entry['created_utc'])\n",
    "                score = entry['score']\n",
    "                pinned = entry['stickied']\n",
    "\n",
    "                cur.execute(\"\"\"INSERT INTO reddit(\n",
    "                    id,\n",
    "                    author,\n",
    "                    comment,\n",
    "                    score,\n",
    "                    pinned,\n",
    "                    created_at) values(?,?,?,?,?, ?)\"\"\",(\n",
    "                    id,\n",
    "                    author,\n",
    "                    comment_txt,\n",
    "                    score,\n",
    "                    pinned,\n",
    "                    created_at)\n",
    "                )\n",
    "                \n",
    "                conn.commit()\n",
    "                print(comment_txt[:20])\n",
    "                display.clear_output(wait=True)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print('Error text:', e)\n",
    "\n",
    "subreddit = 'politics'\n",
    "size = 100\n",
    "capitol_day = datetime.datetime.strptime('21/01/2021',\"%d/%m/%Y\")\n",
    "\n",
    "# minutes from midnight 21/01/2021, we wanted to scrape 6 hours hence 360 minutes, since our function gets 100 entries for each minute timeframe.\n",
    "delta_limit_up = 360\n",
    "delta_limit_low = 359\n",
    "\n",
    "for i in range(360): # range decides the amount of minutes to go from upper timestamp - capitol day\n",
    "    previous_timestamp = int((capitol_day - datetime.timedelta(minutes=delta_limit_up)).timestamp())\n",
    "    current_timestamp = int((capitol_day - datetime.timedelta(minutes=delta_limit_low)).timestamp())\n",
    "\n",
    "    load_results(previous_timestamp, current_timestamp, subreddit, size)\n",
    "            \n",
    "    delta_limit_up -= 1\n",
    "    delta_limit_low -= 1\n",
    "    \n",
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "bb0d9967-26a1-48fb-8b86-92a9246da9f4",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
