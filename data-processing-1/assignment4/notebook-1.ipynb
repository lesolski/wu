{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2120badbc98de39b433d8e3b7abaacaf",
     "grade": false,
     "grade_id": "cell-9e78272cc4af091a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*General hints:* <br>\n",
    "* You may use another notebook to test different approaches and ideas. When complete and mature, turn your code snippets into the requested functions in this notebook for submission. \n",
    "* Make sure the function implementations are generic and can be applied to any dataset (not just the one provided).\n",
    "* Add explanatory code comments in the code cells. Make sure that these comments improve our understanding of your implementation decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca384ab2b432240f2e75945bb1878249",
     "grade": false,
     "grade_id": "cell-9d52cb434e7f0910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "* Create a variable holding your student id, as shown below. \n",
    "* Simply replace the example (`01234567`) with your actual student id having a total of 8 digits. \n",
    "* Maintain the variable as a string, do NOT change its type in this notebook!\n",
    "* *Note: If your student id has 7 digits, add a leading 0. The final student id MUST have 8 digits!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = '01453741'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "744e924b458ceac8c8906d78f4042f98",
     "grade": false,
     "grade_id": "cell-420b4c449379ffe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0. Import\n",
    "\n",
    "Implement a function `tidy` which imports the data set assigned and provided to you as a CSV file into a `pandas` dataframe. Access the data set and establish whether your data set is tidy. If not, clean the data set before continuing with Step 1. Mind all rules of tidying data sets in this step. Make sure you comply to the following statements:\n",
    "* If there is an index column in your dataset, keep it.\n",
    "* It is ok that one person has multiple jobs. You do not need to split those.\n",
    "* The dataset should have a total of 8 columns (not including the index), the first column should be `full_name`.\n",
    "* Mind the intended content of each attribute (e.g., `full_name` should contain the full name of a person, no need to change that)\n",
    "* Have the function `tidy` return the ready data set as a dataframe.\n",
    "\n",
    "Note that `tidy` must take a single parameter that holds the basename of the CSV file (i.e., the name without file extension). Do NOT change the name of the file, do not overwrite the original data file, and make sure you submit your final ZIP following the [Code of Conduct](https://datascience.ai.wu.ac.at/ws20/dataprocessing1/code_of_conduct.html) requirements. Especially, make sure you put your data file in a folder called `data/` when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb72cc2bb4a391bd34348f7ca73272a8",
     "grade": false,
     "grade_id": "cell-81e21dccd785e63d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>automotive</th>\n",
       "      <th>color</th>\n",
       "      <th>job</th>\n",
       "      <th>address</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>company_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Santos</td>\n",
       "      <td>670 SJV</td>\n",
       "      <td>white smoke</td>\n",
       "      <td>Horticulturist, commercial</td>\n",
       "      <td>Lake Dustin</td>\n",
       "      <td>(29.2588815, 68.330046)</td>\n",
       "      <td>Pham Inc</td>\n",
       "      <td>2020-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brandi Castillo</td>\n",
       "      <td>89-22271</td>\n",
       "      <td>medium aqua marine</td>\n",
       "      <td>Ambulance person</td>\n",
       "      <td>Blanchardtown</td>\n",
       "      <td>(-57.8942475, -126.431194)</td>\n",
       "      <td>Ashley and Michael</td>\n",
       "      <td>2020-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paula Dodson</td>\n",
       "      <td>L62 8UK</td>\n",
       "      <td>sky blue</td>\n",
       "      <td>Pension scheme manager</td>\n",
       "      <td>Port Karenfurt</td>\n",
       "      <td>(50.146492, -157.040417)</td>\n",
       "      <td>Castro, Meyer and Smith</td>\n",
       "      <td>2020-05-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barbara Garner</td>\n",
       "      <td>804LCR</td>\n",
       "      <td>dark slate gray</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(16.274987, -59.948235)</td>\n",
       "      <td>Hughes and Bowman</td>\n",
       "      <td>2020-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Teresa Allen</td>\n",
       "      <td>9JW04</td>\n",
       "      <td>dark salmon</td>\n",
       "      <td>Industrial/product designer</td>\n",
       "      <td>Meganland</td>\n",
       "      <td>(-8.875462, -57.949751)</td>\n",
       "      <td>Bennett and Wilson</td>\n",
       "      <td>2020-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>Lisa Williams</td>\n",
       "      <td>1-C4003</td>\n",
       "      <td>lavender</td>\n",
       "      <td>Jewellery designer</td>\n",
       "      <td>Sawyerstad</td>\n",
       "      <td>(15.909499, -175.838439)</td>\n",
       "      <td>Wood Group</td>\n",
       "      <td>2020-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>Julie Schaefer</td>\n",
       "      <td>GJ3 5912</td>\n",
       "      <td>cornflower blue</td>\n",
       "      <td>Research officer, government</td>\n",
       "      <td>Jessicaville</td>\n",
       "      <td>(-83.589762, -52.721594)</td>\n",
       "      <td>Bryan LLC</td>\n",
       "      <td>2020-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>Michelle Lane</td>\n",
       "      <td>406TIE</td>\n",
       "      <td>crimson</td>\n",
       "      <td>Manufacturing engineer</td>\n",
       "      <td>Donaldsonside</td>\n",
       "      <td>(-78.7827135, 147.130558)</td>\n",
       "      <td>Lyons and Perez</td>\n",
       "      <td>2020-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>Cheryl Rowe</td>\n",
       "      <td>JVX G25</td>\n",
       "      <td>ghost white</td>\n",
       "      <td>Teacher, special educational needs</td>\n",
       "      <td>Gateshaven</td>\n",
       "      <td>(42.5015945, -11.491344)</td>\n",
       "      <td>Bowers and Miller</td>\n",
       "      <td>2020-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>Samuel Oconnor</td>\n",
       "      <td>QKC 584</td>\n",
       "      <td>coral</td>\n",
       "      <td>Curator</td>\n",
       "      <td>West Sarahfurt</td>\n",
       "      <td>(38.4488755, 177.341533)</td>\n",
       "      <td>Patterson and Houston</td>\n",
       "      <td>2020-03-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1711 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            full_name automotive               color  \\\n",
       "0       Rachel Santos    670 SJV         white smoke   \n",
       "1     Brandi Castillo   89-22271  medium aqua marine   \n",
       "2        Paula Dodson    L62 8UK            sky blue   \n",
       "3      Barbara Garner     804LCR     dark slate gray   \n",
       "4        Teresa Allen      9JW04         dark salmon   \n",
       "...               ...        ...                 ...   \n",
       "1706    Lisa Williams    1-C4003            lavender   \n",
       "1707   Julie Schaefer   GJ3 5912     cornflower blue   \n",
       "1708    Michelle Lane     406TIE             crimson   \n",
       "1709      Cheryl Rowe    JVX G25         ghost white   \n",
       "1710   Samuel Oconnor    QKC 584               coral   \n",
       "\n",
       "                                     job         address  \\\n",
       "0             Horticulturist, commercial     Lake Dustin   \n",
       "1                       Ambulance person   Blanchardtown   \n",
       "2                 Pension scheme manager  Port Karenfurt   \n",
       "3                         Data scientist             NaN   \n",
       "4            Industrial/product designer       Meganland   \n",
       "...                                  ...             ...   \n",
       "1706                  Jewellery designer      Sawyerstad   \n",
       "1707        Research officer, government    Jessicaville   \n",
       "1708              Manufacturing engineer   Donaldsonside   \n",
       "1709  Teacher, special educational needs      Gateshaven   \n",
       "1710                             Curator  West Sarahfurt   \n",
       "\n",
       "                     coordinates             company_name        date  \n",
       "0        (29.2588815, 68.330046)                 Pham Inc  2020-09-05  \n",
       "1     (-57.8942475, -126.431194)       Ashley and Michael  2020-07-09  \n",
       "2       (50.146492, -157.040417)  Castro, Meyer and Smith  2020-05-22  \n",
       "3        (16.274987, -59.948235)        Hughes and Bowman  2020-06-10  \n",
       "4        (-8.875462, -57.949751)       Bennett and Wilson  2020-10-19  \n",
       "...                          ...                      ...         ...  \n",
       "1706    (15.909499, -175.838439)               Wood Group  2020-04-06  \n",
       "1707    (-83.589762, -52.721594)                Bryan LLC  2020-09-19  \n",
       "1708   (-78.7827135, 147.130558)          Lyons and Perez  2020-04-08  \n",
       "1709    (42.5015945, -11.491344)        Bowers and Miller  2020-04-28  \n",
       "1710    (38.4488755, 177.341533)    Patterson and Houston  2020-03-14  \n",
       "\n",
       "[1711 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from decimal import Decimal # used for cleaning coordinates column, to convert them to float numbers which is much more convenient\n",
    "from IPython.display import display # used for displaying dataframes like table in Jupyter env.\n",
    "\n",
    "miss_chars = ['NA', '-inf', 'inf', 'nan', None, 'None','', ' ', 0, 'NaN'] # possible missing values\n",
    "\n",
    "\n",
    "\n",
    "def tidy(x):\n",
    "    \"\"\"Tidying-up the dataframe\"\"\"\n",
    "    x = str(x) + '.csv'\n",
    "    path = os.path.join('data', x) # system independet path, so it wont fail across systems\n",
    "    df = pd.read_csv(path, sep=',', na_values = miss_chars)\n",
    "    df = df.T # transposing the df\n",
    "    df = df.rename(columns={0: 'full_name', 1:'automotive', 2:'color', 3:'job', 4:'address', 5:'coordinates', 6:'company_name'}) # adding header\n",
    "    \n",
    "    \n",
    "    def find_comp(comp):\n",
    "        \"\"\"Extracts company name from messed-up column\"\"\"\n",
    "        result = re.sub(r'[^a-zA-Z \\-,]+', '', comp) # extracting \n",
    "        result = result.lstrip('-').replace('-', ' and ') # removing '-' that is left on left side of the string from Date and replacing '-' left between names with ' and '' because that's how I think it should be done based on other company names.\n",
    "        return result\n",
    "    \n",
    "    def check_date(date):\n",
    "        \"\"\"Extracts Date from messed-up column\"\"\"\n",
    "        result = re.findall('((?:19|20)\\\\d\\\\d)-(0?[1-9]|1[012])-([12][0-9]|3[01]|0?[1-9])', date) # regex that extracts only valid dates from a string\n",
    "        if len(result) == 1:\n",
    "            return '-'.join(result[0]) # joining tuple\n",
    "        else:\n",
    "            return 'NaN' # if it's not valid date it will only return 'NaN'\n",
    "    \n",
    "    def clean_coords(cords):\n",
    "        \"\"\"Extracts only numbers from Decimal() in coordinates column\"\"\"\n",
    "        if isinstance(cords, str):\n",
    "            res = eval(cords) # converting string to tuple\n",
    "            if isinstance(res, tuple):\n",
    "                return float(res[0]), float(res[1]) # returning new tuple that contains only numbers\n",
    "            else:\n",
    "                return np.NaN # returns NaN if coordinates column failes to meet conditions for reformatting\n",
    "        else:\n",
    "            return cords\n",
    "        \n",
    "    def clean_address(add): # removing values with only numbers from address column since these are irrelevant\n",
    "        \"\"\"If only numbers are present, it will replace it with NaN\"\"\"\n",
    "        try:\n",
    "            add_check = int(add)\n",
    "            return np.NaN\n",
    "        except:\n",
    "            return add\n",
    "\n",
    "        \n",
    "    def clean_color(color):\n",
    "        \"\"\"Splits color names and makes them lowercase so they look more natural\"\"\"\n",
    "        if isinstance(color, str):\n",
    "            result = re.findall('[A-Z][^A-Z]*', color)\n",
    "            return ' '.join(result).lower() # returns lowercase whitespace devided color names\n",
    "        else:\n",
    "            return np.NaN\n",
    "\n",
    "        \n",
    "    # applying all of above functions to dataframe\n",
    "    df['date'] = df['company_name'].apply(lambda date: check_date(date)) # making new column with extracted dates\n",
    "    df['company_name'] = df['company_name'].apply(lambda company: find_comp(company)) # cleaning column so it only keeps company names\n",
    "    df['coordinates'] = df['coordinates'].apply(lambda coords: clean_coords(coords)) # cleaning coordinates column\n",
    "    df['address'] = df['address'].apply(lambda add: clean_address(add)) # cleaning addresss column\n",
    "    df['color'] = df['color'].apply(lambda color: clean_color(color)) # cleaning color column\n",
    "    \n",
    "    df.replace(miss_chars, np.NaN, inplace=True) # replacing all possible 'missing characters' with np.NaN so they can be read with dataFrame.isna() or dataFrame.isnull() *really important for next two parts*\n",
    "    \n",
    "    return df\n",
    "\n",
    "display(tidy(mn)) # displaying the dataframe in IPython manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21071c01488e30d64223a1d4cdc68565",
     "grade": true,
     "grade_id": "cell-9a75a2763c7bbe5d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "import pandas\n",
    "assert_equal(type(tidy(mn)), pandas.core.frame.DataFrame)\n",
    "assert_equal(len((tidy(mn)).columns), 8)\n",
    "assert_equal(list((tidy(mn)).columns)[0], \"full_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f557e87213383ef517513b7eb10c818e",
     "grade": false,
     "grade_id": "cell-72c2573051ab8535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-------\n",
    "## 1. Missing values\n",
    "\n",
    "### 1.1 Code part\n",
    "Implement a function called `missing_values` which takes as an input a dataframe and check whether there are any missing values in the dataset. Record the row ids of the observations containing missing values as a list of numbers and make sure that the function returns the recorded list in the end. If there are no missing values, `missing_values` should return an empty list. Mind the following:\n",
    "\n",
    "* Missing values may be encoded using any or multiple of the following special-purpose values: `NA`, `-inf`, `inf`, `nan`, `None`, ` `, `0`, or the empty string.\n",
    "* There will be at least one, but likely many more missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c924e80de70e8169cc282686502fb094",
     "grade": false,
     "grade_id": "cell-6d4fb7f2e44e7cfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '12', '27', '41', '57', '109', '119', '131', '140', '158', '167', '182', '224', '227', '268', '273', '287', '308', '331', '337', '366', '383', '385', '387', '409', '413', '433', '457', '487', '494', '507', '512', '529', '532', '556', '608', '628', '645', '651', '685', '703', '750', '766', '806', '808', '816', '840', '853', '883', '939', '953', '959', '976', '1016', '1034', '1051', '1089', '1105', '1141', '1172', '1176', '1180', '1254', '1276', '1288', '1297', '1321', '1330', '1339', '1354', '1362', '1371', '1378', '1441', '1463', '1484', '1520', '1558', '1580', '1584', '1621', '1641', '1674', '1691', '1697']\n"
     ]
    }
   ],
   "source": [
    "def missing_values(df): \n",
    "    miss_chars = ['NA', '-inf', 'inf', 'nan', None, 'None','', ' ', 0, 'NaN']\n",
    "    df.replace(miss_chars, np.NaN, inplace=True)\n",
    "    list_of_miss_idx = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row.isna().any() == True:\n",
    "            list_of_miss_idx.append(index)\n",
    "    return list_of_miss_idx\n",
    "\n",
    "\n",
    "print(missing_values(tidy(mn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c9c89cfb320ba834787d78861732148",
     "grade": true,
     "grade_id": "cell-4c692eab5b638fc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "from nose.tools import assert_true\n",
    "assert_equal(type(missing_values(tidy(mn))), list)\n",
    "assert_true(len(missing_values(tidy(mn))) > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52fac7bcbfdfcaea74bdcda572dac51b",
     "grade": false,
     "grade_id": "cell-82f316abfa9423e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Analytical part\n",
    "\n",
    "* Does the dataset contain missing values?\n",
    "* If no, explain how you proved that this is actually the case.\n",
    "* If yes, describe the discovered missing values. What could be an explanation for their missingness?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229a22e8b0e33f07eb7e3e7a6a652bf2",
     "grade": true,
     "grade_id": "cell-3ad38c6f2d1998f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "By running `print(len(missing_values(tidy(mn))` I can see how many values are missing. In my case it is 85 values. (`print(df.isna().sum().sum())` also does the same job)\n",
    "\n",
    "There are missing values in every column. I collected all indexes and run  \n",
    "`for row_idx in missing:\n",
    "     display(df.loc[[str(row_idx)]])` to check/see them manually. \n",
    "  \n",
    "With running following code on dataFrame I can see how many missing values I have per column.  \n",
    "`for column in df.columns:\n",
    "    result = df[column].isnull().sum()\n",
    "    print(column, result)` This can also be done with `df.isna().sum()` which is much more simpler and gives cleaner output.  \n",
    "\n",
    "full_name        9  \n",
    "automotive       5  \n",
    "color           11  \n",
    "job              8  \n",
    "address         20  \n",
    "coordinates     12  \n",
    "company_name    10  \n",
    "date            10  \n",
    "dtype: int64  \n",
    "\n",
    "By running `print(round(df.isna().count().sum() / df.count().sum(), 2), '%')` I get 1.01% as output, which means only around 1% of all values are missing in my dataset. Same can be done for each column, but I think there is no need for that right now.\n",
    "\n",
    "There are many possible cases why values are missing in dataset. But I would say, that in this case they were not inputed correctly in the database or because some of the data was generated automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c24f0d48c1dd572520c046b018230c4b",
     "grade": false,
     "grade_id": "cell-87126fc406d941ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## 2. Handling missing values\n",
    "### 2.1. Code part\n",
    "Implement a function called *handling_missing_values* for handling all types of missing values and all their occurrences in our data set. For each missing-value type, and the corresponding variable, choose an appropriate strategy. Make use of the techniques learned in Unit 4. Do NOT simply drop the missing values. Do NOT apply a single technique only. The function should take as an input a dataframe holding missings and return the updated dataframe w/o missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0226044c2b66de952ca20bc64edae12b",
     "grade": false,
     "grade_id": "cell-5ba2dc8b5cbe1f8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def handling_missing_values(x):\n",
    "    \n",
    "    # since I know from previous step that I have missing values in each column I will have to deal with each of them separately\n",
    "    \n",
    "    x['full_name'].fillna(method='ffill', inplace=True) # I would say that full_name column doesn't have much impact\n",
    "#     x.dropna(subset=['automotive'], inplace=True) # I consider automotive as one of things that should be unique so I will drop rows that dont have value in automotve column. Well test doesn't allow dropping. :(\n",
    "    x['automotive'].fillna(0, inplace=True) # not useful at all\n",
    "    x['color'].fillna(x['color'].value_counts().index[0], inplace=True) # I will replace color with most common one since I consider it is not very important\n",
    "    \n",
    "    # For all other columns I will just use imputation with LOCF and one bfill\n",
    "    x['job'].fillna(method='ffill', inplace=True)\n",
    "    x['date'].fillna(method='ffill', inplace=True)\n",
    "    x['company_name'].fillna(method='ffill', inplace=True)\n",
    "    x['address'].fillna(method='ffill', inplace=True)\n",
    "    x['coordinates'].fillna(method='bfill', inplace=True) #  uses next valid observation to fill the gap\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d55a61422e9e359dc2bc40facbb24a10",
     "grade": true,
     "grade_id": "cell-0edce052e98e2e95",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(len(missing_values(handling_missing_values(tidy(mn)))), 0)\n",
    "assert_equal(handling_missing_values(tidy(mn)).shape, tidy(mn).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f0e74afd6544c0ffa7a9632fab9455a",
     "grade": false,
     "grade_id": "cell-c697641b9d5a1c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2. Analytical part\n",
    "Discuss the implications. What are the benefits and disadvantages of the adopted strategies?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8def9ebc41d3525288f57d8986176fca",
     "grade": true,
     "grade_id": "cell-5c05456587f2ff17",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I implemented basic techniques like mode, LOCF. Since I have a lot of categorical values I wasn't really able to implement any other tehniques.  \n",
    "With LOCF I might have created some duplicates and I think that could be a potential disadvantage of choosing this strategy.  \n",
    "Since only 1% of all values are missing, I think that this approach is fair enough and would get the job done.   \n",
    "I also think that choosing the right strategy depends on the task that should be done with this dataset. \n",
    "\n",
    "I wasn't in good health last 2 weeks, so I wasn't able to dive deeper into this matter of choosing the right strategies. But, will definitely do that. Apologies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfaf0570a8a70924b35ef279df754c19",
     "grade": false,
     "grade_id": "cell-573d56d6699b84eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 3. Duplicate entries\n",
    "Implement a function called `duplicates` that takes as an input a (tidy) dataframe `x`. Assume that `duplicates` receives a dataframe as returned from your Step 0 implementation of `tidy`. It then checks whether there are any duplicates in the dataset. Record the row ids of the observations being duplicates and have `duplicates` returns the list in the end. An empty list indicates the absence of duplicated observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d3199cb48685917455f085c8d366844",
     "grade": false,
     "grade_id": "cell-7954cffea933a812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15', '23', '50', '55', '69', '81', '104', '114', '116', '142', '146', '156', '170', '200', '243', '247', '250', '257', '261', '284', '302', '305', '319', '340', '362', '372', '379', '428', '448', '464', '498', '503', '515', '537', '564', '570', '574', '598', '634', '639', '653', '697', '714', '717', '730', '738', '769', '776', '798', '823', '827', '834', '838', '842', '880', '926', '929', '941', '945', '947', '963', '966', '985', '987', '1001', '1009', '1019', '1022', '1032', '1036', '1039', '1056', '1065', '1075', '1083', '1099', '1101', '1118', '1131', '1147', '1231', '1238', '1246', '1265', '1300', '1313', '1332', '1343', '1357', '1367', '1383', '1388', '1391', '1405', '1413', '1425', '1431', '1434', '1446', '1456', '1459', '1475', '1522', '1537', '1554', '1572', '1576', '1595', '1607', '1630', '1633']\n"
     ]
    }
   ],
   "source": [
    "def duplicates(x):\n",
    "    df = x[x.duplicated(subset=['automotive', 'full_name'],keep='last')]\n",
    "    return [idx for idx in df.index] # returning list of row indexes of duplicates\n",
    "\n",
    "print(duplicates(tidy(mn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81f1ddaa2595165edcbe3c3848bf1880",
     "grade": true,
     "grade_id": "cell-583bc8ab4ba38aa6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(type(duplicates(tidy(mn))), list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8078773ced981185cbbd33775ad95033",
     "grade": false,
     "grade_id": "cell-b04e3f6689a78a44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 4. Handling duplicate entries\n",
    "### 4.1. Code part\n",
    "Implement a function called `handling_duplicate_entries` for handling duplicate entries. Again, the function is assumed to receive a tidied data set as obtained from Step 0. It deduplicates the tidy data set. The function then returns the dataframe without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d48a0cd4a7b84e1aa63f4c5cbcb16630",
     "grade": false,
     "grade_id": "cell-f593e79eae8f4227",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def handling_duplicate_entries(x):\n",
    "    x.drop_duplicates(subset=['automotive', 'full_name'], keep='last', inplace=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10bdb20c5d61c818cc5ff269c76ccb62",
     "grade": true,
     "grade_id": "cell-231217b55b6ef843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(len(duplicates(handling_duplicate_entries(tidy(mn)))), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50f4323a62aba345238646137d3fa80f",
     "grade": false,
     "grade_id": "cell-8c4530b128b5c186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. Analytical part\n",
    "Discuss the implications. What are the benefits and disadvantages of the chosen deduplication strategy?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e93305db24e2fc3800f252b797c2ad7",
     "grade": true,
     "grade_id": "cell-254db63097c3ed40",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I consider combination of full_name and automotive to be unique key, so I will find all rows with duplicate values of these two columns combined and only keep last occurrence of them. Because I think the last one is the updated one. Next I will find all other duplicates that contain same values in all columns. I will also keep first occurrence of these duplicates, so I will keep that data in the dataset. Advantage of this strategy is that I will remove all previously inputed data that might have been changed, so I will have only updated data. When it comes to rows that are completely same as some other rows in this dataset I would say that only one occurrence is enough. In some other datasets, rows might be the same but still not be the duplicates. I don't know any example yet, but parameter keep=False in drop_cuplicates function suggests that this is possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
